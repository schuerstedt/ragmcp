{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3Crc_qvjnc3"
      },
      "source": [
        "# **Naive RAG**\n",
        "The Naive RAG is the simplest technique in the RAG ecosystem, providing a straightforward approach to combining retrieved data with LLM models for efficient user responses.\n",
        "\n",
        "Research Paper: [RAG](https://arxiv.org/pdf/2005.11401)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **📚 Notebook Summary & Step-by-Step Guide** \n",
        "\n",
        "## **🎯 What This Notebook Does**\n",
        "This notebook implements a **Naive RAG (Retrieval Augmented Generation)** system using Azure OpenAI and FAISS. It demonstrates how to:\n",
        "- Load and process CSV data into a vector database\n",
        "- Perform semantic search on documents\n",
        "- Generate AI responses based on retrieved context\n",
        "- Handle Windows encoding issues and Azure OpenAI rate limits\n",
        "\n",
        "## **🔧 Key Libraries & Their Roles**\n",
        "\n",
        "### **Core RAG Libraries**\n",
        "- **`langchain`** - Main orchestration framework for RAG pipelines\n",
        "- **`langchain-community`** - Extended components (CSV loader, FAISS integration)\n",
        "- **`langchain-openai`** - Azure OpenAI integration for embeddings and chat\n",
        "\n",
        "### **Vector Storage & Search**\n",
        "- **`faiss-cpu`** - Facebook AI Similarity Search - fast vector similarity search\n",
        "- **`pandas`** - Data manipulation and CSV handling (fallback option)\n",
        "\n",
        "### **Azure OpenAI Components**\n",
        "- **`AzureOpenAIEmbeddings`** - Converts text to 1536-dimensional vectors\n",
        "- **`AzureChatOpenAI`** - GPT-4o-mini for generating responses\n",
        "\n",
        "### **Document Processing**\n",
        "- **`CSVLoader`** - Loads CSV files into LangChain Document format\n",
        "- **`RecursiveCharacterTextSplitter`** - Splits large documents into chunks\n",
        "\n",
        "## **📋 Step-by-Step Process**\n",
        "\n",
        "### **Step 1: Environment Setup**\n",
        "- Configure Azure OpenAI credentials (API key, endpoint, version)\n",
        "- Set up local development environment (Windows compatibility)\n",
        "\n",
        "### **Step 2: Document Loading & Processing**\n",
        "- Load CSV data using `CSVLoader` with UTF-8 encoding (Windows fix)\n",
        "- Limit documents to 20 to avoid Azure rate limits\n",
        "- Split documents into 500-character chunks for better retrieval\n",
        "\n",
        "### **Step 3: Vector Database Creation**\n",
        "- Convert document chunks to embeddings using Azure OpenAI `text-embedding-ada-002`\n",
        "- Store embeddings in FAISS in-memory vector database\n",
        "- Explore vectorstore structure for educational understanding\n",
        "\n",
        "### **Step 4: RAG Pipeline Setup**\n",
        "- Create retriever from FAISS vectorstore\n",
        "- Set up Azure ChatGPT (gpt-4o-mini) for response generation\n",
        "- Build RAG chain: Query → Retrieve Context → Generate Response\n",
        "\n",
        "### **Step 5: Testing & Validation**\n",
        "- Test with targeted questions about document content\n",
        "- Demonstrate how similarity search finds relevant context\n",
        "- Show end-to-end RAG response generation\n",
        "\n",
        "### **Step 6: Evaluation Preparation**\n",
        "- Structure data for evaluation frameworks\n",
        "- Prepare datasets for quality assessment (Athina AI integration commented out)\n",
        "\n",
        "## **🚀 Key Innovations in This Implementation**\n",
        "- **Windows Compatibility**: UTF-8 encoding fix for Unicode issues\n",
        "- **Rate Limit Management**: Document limiting to prevent Azure quota exhaustion  \n",
        "- **Educational Features**: Vectorstore exploration for learning purposes\n",
        "- **Azure Integration**: Full Azure OpenAI stack instead of standard OpenAI\n",
        "- **Error Handling**: Robust fallback mechanisms and debugging\n",
        "\n",
        "## **💡 Learning Outcomes**\n",
        "Students will understand:\n",
        "- How text becomes vectors and enables semantic search\n",
        "- The role of each component in the RAG pipeline\n",
        "- Practical considerations for production deployment\n",
        "- Azure OpenAI integration and configuration\n",
        "- Vector database concepts and similarity search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQuZ2ARQks63"
      },
      "source": [
        "## **Initial Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XktcxUHHi0FF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
            "  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting langsmith>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.4.14-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/mac/miniconda/lib/python3.12/site-packages (from langchain) (2.10.3)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /home/mac/miniconda/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
            "Collecting PyYAML>=5.3 (from langchain)\n",
            "  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
            "  Downloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain-community)\n",
            "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting numpy>=1.26.2 (from langchain-community)\n",
            "  Downloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Collecting openai<2.0.0,>=1.99.9 (from langchain-openai)\n",
            "  Downloading openai-1.99.9-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading multidict-6.6.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/mac/miniconda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /home/mac/miniconda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.12.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /home/mac/miniconda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.2)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson>=3.9.14 (from langsmith>=0.1.17->langchain)\n",
            "  Downloading orjson-3.11.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
            "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /home/mac/miniconda/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Collecting anyio<5,>=3.5.0 (from openai<2.0.0,>=1.99.9->langchain-openai)\n",
            "  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/mac/miniconda/lib/python3.12/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.9.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.99.9->langchain-openai)\n",
            "  Downloading jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting sniffio (from openai<2.0.0,>=1.99.9->langchain-openai)\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: tqdm>4 in /home/mac/miniconda/lib/python3.12/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/mac/miniconda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /home/mac/miniconda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mac/miniconda/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mac/miniconda/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mac/miniconda/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mac/miniconda/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
            "  Downloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain-openai)\n",
            "  Downloading regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
            "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /home/mac/miniconda/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.30-py3-none-any.whl (74 kB)\n",
            "Downloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading langchain_core-0.3.74-py3-none-any.whl (443 kB)\n",
            "Downloading langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\n",
            "Downloading langsmith-0.4.14-py3-none-any.whl (373 kB)\n",
            "Downloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.99.9-py3-none-any.whl (786 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m786.8/786.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading anyio-4.10.0-py3-none-any.whl (107 kB)\n",
            "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
            "Downloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (607 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.6/607.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Downloading jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
            "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading multidict-6.6.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
            "Downloading orjson-3.11.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
            "Downloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (224 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.9/801.9 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: typing-inspection, tenacity, sniffio, regex, PyYAML, python-dotenv, propcache, orjson, numpy, mypy-extensions, multidict, marshmallow, jiter, httpx-sse, h11, greenlet, frozenlist, attrs, aiohappyeyeballs, yarl, typing-inspect, tiktoken, SQLAlchemy, requests-toolbelt, httpcore, anyio, aiosignal, pydantic-settings, httpx, dataclasses-json, aiohttp, openai, langsmith, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
            "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.43 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 anyio-4.10.0 attrs-25.3.0 dataclasses-json-0.6.7 frozenlist-1.7.0 greenlet-3.2.4 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.1 jiter-0.10.0 langchain-0.3.27 langchain-community-0.3.27 langchain-core-0.3.74 langchain-openai-0.3.30 langchain-text-splitters-0.3.9 langsmith-0.4.14 marshmallow-3.26.1 multidict-6.6.4 mypy-extensions-1.1.0 numpy-2.3.2 openai-1.99.9 orjson-3.11.2 propcache-0.3.2 pydantic-settings-2.10.1 python-dotenv-1.1.1 regex-2025.7.34 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 tiktoken-0.11.0 typing-inspect-0.9.0 typing-inspection-0.4.1 yarl-1.20.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# ! pip install --q athina langchain-openai\n",
        "%pip install langchain langchain-community langchain-openai pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Get the parent directory of the current working directory (where the notebook is running)\n",
        "parent_dir = Path.cwd().parent\n",
        "env_path = parent_dir / \".env\"\n",
        "\n",
        "# Load the .env file from the parent directory\n",
        "load_dotenv(dotenv_path=env_path, override=True)\n",
        "\n",
        "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
        "AzureOpenAIEmbeddingsModel = os.getenv(\"AzureOpenAIEmbeddingsModel\", \"text-embedding-ada-002\")\n",
        "AzureChatOpenAIModel = os.getenv(\"AzureChatOpenAIModel\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAzEvcXulGYW"
      },
      "outputs": [],
      "source": [
        "#import os\n",
        "# from google.colab import userdata  # Uncomment if using Google Colab\n",
        "\n",
        "# Azure OpenAI Configuration\n",
        "# For local development, set these environment variables directly or use a .env file\n",
        "#os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv('AZURE_OPENAI_API_KEY', 'Your_Azure_OpenAI_API_Key_Here')\n",
        "#os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv('AZURE_OPENAI_ENDPOINT', 'https://your-resource-name.openai.azure.com/')\n",
        "#os.environ[\"OPENAI_API_VERSION\"] = \"2024-02-01\"  # or your preferred API version\n",
        "\n",
        "# Other API keys (optional)\n",
        "#os.environ['ATHINA_API_KEY'] = os.getenv('ATHINA_API_KEY', '')\n",
        "#os.environ['PINECONE_API_KEY'] = os.getenv('PINECONE_API_KEY', '')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT6jhNgulNV3"
      },
      "source": [
        "## **Indexing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DmXqyqQ5lMrk"
      },
      "outputs": [],
      "source": [
        "# load embedding model\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "embeddings = AzureOpenAIEmbeddings(\n",
        "    model=AzureOpenAIEmbeddingsModel,\n",
        "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
        "    api_key=AZURE_OPENAI_API_KEY,\n",
        "    api_version=AZURE_OPENAI_API_VERSION\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-ollama\n",
            "  Downloading langchain_ollama-0.3.6-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting ollama<1.0.0,>=0.5.1 (from langchain-ollama)\n",
            "  Downloading ollama-0.5.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in /home/mac/miniconda/lib/python3.12/site-packages (from langchain-ollama) (0.3.74)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /home/mac/miniconda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/mac/miniconda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/mac/miniconda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /home/mac/miniconda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /home/mac/miniconda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (4.12.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /home/mac/miniconda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (24.2)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /home/mac/miniconda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.10.3)\n",
            "Requirement already satisfied: httpx>=0.27 in /home/mac/miniconda/lib/python3.12/site-packages (from ollama<1.0.0,>=0.5.1->langchain-ollama) (0.28.1)\n",
            "Requirement already satisfied: anyio in /home/mac/miniconda/lib/python3.12/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (4.10.0)\n",
            "Requirement already satisfied: certifi in /home/mac/miniconda/lib/python3.12/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /home/mac/miniconda/lib/python3.12/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /home/mac/miniconda/lib/python3.12/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (3.7)\n",
            "Requirement already satisfied: h11>=0.16 in /home/mac/miniconda/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /home/mac/miniconda/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /home/mac/miniconda/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/mac/miniconda/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /home/mac/miniconda/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.32.3)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /home/mac/miniconda/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/mac/miniconda/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /home/mac/miniconda/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mac/miniconda/lib/python3.12/site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mac/miniconda/lib/python3.12/site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/mac/miniconda/lib/python3.12/site-packages (from anyio->httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (1.3.1)\n",
            "Downloading langchain_ollama-0.3.6-py3-none-any.whl (24 kB)\n",
            "Downloading ollama-0.5.3-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: ollama, langchain-ollama\n",
            "Successfully installed langchain-ollama-0.3.6 ollama-0.5.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U langchain-ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_ollama  import OllamaEmbeddings\n",
        "\n",
        "# Example: use the 'nomic-embed-text' model (or another supported by your Ollama instance)\n",
        "embeddings = OllamaEmbeddings(\n",
        "    model=\"nomic-embed-text\",  # or another embedding model available in your Ollama\n",
        "    base_url=\"http://localhost:11434\"  # default Ollama endpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Downloading pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "  Downloading pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /home/mac/miniconda/lib/python3.12/site-packages (from pandas) (2.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/mac/miniconda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /home/mac/miniconda/lib/python3.12/site-packages (from pandas) (2.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/mac/miniconda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /home/mac/miniconda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/12.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mCollecting tzdata>=2022.7 (from pandas)\n",
            "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /home/mac/miniconda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Installing collected packages: pytz, tzdata, pandas\n",
            "Installing collected packages: pytz, tzdata, pandas\n",
            "Successfully installed pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n",
            "Successfully installed pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking if file exists: True\n",
            "Full path: /mnt/f/LernRAG/ragmcp/data/context.csv\n",
            "\n",
            "CSV shape: (232, 1)\n",
            "Columns: ['context']\n",
            "\n",
            "First 3 rows:\n",
            "                                             context\n",
            "0  ['African immigration to the United States ref...\n",
            "1  [\"Discount points, also called mortgage points...\n",
            "2  ['Interlibrary loan (abbreviated ILL, and some...\n",
            "\n",
            "================================================================================\n",
            "FULL CONTENT OF FIRST ROW:\n",
            "================================================================================\n",
            "\n",
            "📄 Column 'context':\n",
            "----------------------------------------\n",
            "['African immigration to the United States refers to immigrants to the United States who are or were nationals of modern African countries. The term African in the scope of this article refers to geographical or national origins rather than racial affiliation. Between the Immigration and Nationality Act of 1965 and 2017, Sub-Saharan African-born population in the United States grew to 2.1 million people.Sub-Saharan Africans in the United States come from almost all regions in Africa and do not constitute a homogeneous group. They include peoples from different national, linguistic, ethnic, racial, cultural and social backgrounds. As such, US and foreign born Sub-Saharan Africans are distinct from native-born African Americans, many of whose ancestors were involuntarily brought from West and Central Africa to the colonial United States by means of the historic Atlantic slave trade. African immigration is now driving the growth of the Black population in New York City.\\n\\n\\n== Immigration legislation ==\\n\\n\\n=== Citizenship ===\\nIn the 1870s, the Naturalization Act was extended to allow \"aliens, being free white persons and to aliens of African nativity and to persons of African descent\" to acquire citizenship. Immigration from Africa was theoretically permitted, unlike non-white immigration from Asia.\\n\\n\\n=== Quotas enacted between 1921 and 1924 ===\\nSeveral laws enforcing national origins quotas on U.S. immigration were enacted between 1921 and 1924 and were in effect until they were repealed in 1965. While the laws were aimed at restricting the immigration of Jews and Catholics from Southern and Eastern Europe and immigration from Asia, they also impacted African immigrants. The legislation effectively excluded Africans from entering the country.\\nThe Emergency Quota Act of 1921 restricted immigration from a given country to 3% of the number of people from that country living in the U.S. according to the census of 1910. The Immigration Act of 1924, also known as the Johnson-Reed Act, reduced that to 2% of the number of people from that country who were already living in the U.S. in 1890. Under the system, the quota for immigrants from Africa (excluding Egypt) totaled 1,100 (the number was increased to 1,400 under the 1952 McCarran–Walter Act.)  That contrasted to immigrants from Germany, which had a limit of 51,227.\\n\\n\\n=== Repeal of quotas ===\\nThe Immigration and Nationality Act of 1965, also known as the Hart-Celler Act, repealed the national quotas and subsequently there was a substantial increase in the number of immigrants from developing countries, particularly in Africa and Asia. This act also provided a separate category for refugees. The act also provided greater opportunity for family reunification.\\n\\n\\n=== Diversity Immigrant Visa ===\\nThe Diversity Visa Program, or green card lottery, is a program created by the Immigration Act of 1990. It allows people born in countries with low rates of immigration to the United States to obtain a lawful permanent resident status. Each year, 50,000 of those visas are distributed at random. Almost 38% of those visas were attributed to African born immigrants in 2016. African born persons also represent the most numerous group among the applicants since 2013. The application is free of charge, and the requirements in terms of education are either a high school diploma or two years of a professional experience requiring at least two years of training.\\n\\n\\n== Recent migration trends and factors ==\\n\\nThe continent of Africa has seen many changes in migrations patterns over the course of history. The graph below shows African immigration to the United States in 2016 based on class of admission with numbers from the Department of Homeland Security\\'s Yearbook.\\nThe influx of African immigrants began in the latter part of the 20th century and is often referred to as the \"fourth great migration.\" About three-fourths of all out-migration from Africa went to the United States after 1990.[7] This trend began after decolonization, as many Africans moved to the U.S. seeking an education and to escape poverty, and has risen steadily over time. The increase in the rate of migration is projected to continue for the coming decades. Originally, these immigrants came with the sole purpose of advancing themselves before returning to their respective countries. Nevertheless, many immigrants never return. In recent'\n",
            " 'many changes in migrations patterns over the course of history. The graph below shows African immigration to the United States in 2016 based on class of admission with numbers from the Department of Homeland Security\\'s Yearbook.\\nThe influx of African immigrants began in the latter part of the 20th century and is often referred to as the \"fourth great migration.\" About three-fourths of all out-migration from Africa went to the United States after 1990.[7] This trend began after decolonization, as many Africans moved to the U.S. seeking an education and to escape poverty, and has risen steadily over time. The increase in the rate of migration is projected to continue for the coming decades. Originally, these immigrants came with the sole purpose of advancing themselves before returning to their respective countries. Nevertheless, many immigrants never return. In recent years there has been an increase in the number of African immigrants interested in gaining permanent residence in the U.S.; this has led to a severe brain drain on the economies of African countries due to many skilled hard-working Africans leaving Africa to seek their economic fortunes in the U.S. and elsewhere.\\nOne major factor that contributes to migration from Africa to the United States is labor opportunities. It has been relatively easier for African immigrants with advanced education to leave and enter international labor markets. In addition, many Africans move to the United States for advanced training. For example, doctors from different African nations would move to the U.S. in order to gain more economic opportunities compared to their home country.[14] However, as more Africans emigrate to the United States, their reasoning and factors tend to become more complex.[15]\\nMany Africans who migrate to the United States return their income to Africa in the source of remittances. In Nigeria, for example, remittances from Nigerians in the United States to Nigeria totaled to $6.1 billion in 2012, approximately 3% of Nigeria\\'s GDP. The important role of remittances in improving the lives of family members in the United States has led to both migration and migrants remaining in the United States.\\nFollowing educational and economic trends of migration, family reunification has driven recent trends of migration. Family reunification refers to the ability of U.S. citizens to sponsor family members for immigration. Sponsoring immediate family members and other family preferences led to 45% and 10% of all African immigration in 2016 respectively. Legal service organizations such as the African Advocacy Network aid in family members sponsoring new immigrants to the United States.Additionally, refugees make up a large class of admission to the United States. Recent crises in the Central African Republic, South Sudan, Nigeria, and Burundi have been sources of migrants in recent years. With recent restrictions on refugee entrance to the United States, refugees may face a harder time entering the United States.\\n\\n\\n== Population ==\\n\\n\\n== Demographics ==\\nThe total immigrant population from Africa estimated for 2015-2019 was 2,256,700. The top counties were:\\n1) Harris County, TX ----------------------------- 70,200\\n2) Los Angeles County, CA ------------------- 59,900\\n3) Bronx Borough, NY --------------------------- 56,000\\n4) Montgomery County, MD ----------------- 54,700\\n5) Hennepin County, MN ---------------------- 53,700\\n6) Prince George\\'s County, MD ------------- 53,000\\n7) Dallas County, TX ----------------------------- 46,700\\n8) King County, WA ------------------------------- 42,500\\n9) Cook County, IL -------------------------------- 42,100\\n10) Franklin County, OH ----------------------- 41,400\\n11) Fairfax County, VA -------------------------- 39,400\\n12) Tarrant County, TX ------------------------- 34,900\\n13) Brooklyn Borough, NY -------------------- 32,500\\n14) Gwinnett County, GA ---------------------- 26,800\\n15) Essex County, NJ --------------------------- 26,700\\n16) Suffolk County, Mass. -------------------- 25,000\\n17) Philadelphia County, PA ----------------- 25,000\\n18) Queens Borough, NY ---------------------- 24,700\\n19) DeKalb County, GA ------------------------- 24,500\\n20) Maricopa County, AZ ---------------------']\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test CSV file reading and show preview\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "csv_path = \"../data/context.csv\"\n",
        "print(f\"Checking if file exists: {os.path.exists(csv_path)}\")\n",
        "print(f\"Full path: {os.path.abspath(csv_path)}\")\n",
        "\n",
        "if os.path.exists(csv_path):\n",
        "    # Read and display CSV content\n",
        "    df_preview = pd.read_csv(csv_path)\n",
        "    print(f\"\\nCSV shape: {df_preview.shape}\")\n",
        "    print(f\"Columns: {list(df_preview.columns)}\")\n",
        "    print(\"\\nFirst 3 rows:\")\n",
        "    print(df_preview.head(3))\n",
        "    \n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(\"FULL CONTENT OF FIRST ROW:\")\n",
        "    print(\"=\"*80)\n",
        "    if len(df_preview) > 0:\n",
        "        for col in df_preview.columns:\n",
        "            content = str(df_preview[col].iloc[0])\n",
        "            print(f\"\\n📄 Column '{col}':\")\n",
        "            print(\"-\" * 40)\n",
        "            print(content)\n",
        "            print(\"-\" * 40)\n",
        "else:\n",
        "    print(\"❌ CSV file not found at the specified path\")\n",
        "    print(\"Current working directory:\", os.getcwd())\n",
        "    print(\"Files in current directory:\", os.listdir(\".\"))\n",
        "    if os.path.exists(\"../data\"):\n",
        "        print(\"Files in ../data directory:\", os.listdir(\"../data\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lv9mslL9lWdJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 232 documents from CSV\n",
            "Limited to 20 documents to avoid rate limits\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "from langchain_community.document_loaders import CSVLoader\n",
        "\n",
        "# Create CSVLoader with UTF-8 encoding (Windows compatibility)\n",
        "loader = CSVLoader(\n",
        "    file_path=\"../data/context.csv\",\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "# Load documents\n",
        "documents = loader.load()\n",
        "print(f\"Loaded {len(documents)} documents from CSV\")\n",
        "\n",
        "# Limit documents to prevent Azure OpenAI rate limits\n",
        "MAX_DOCUMENTS = 20  # Adjust this number based on your rate limits\n",
        "if len(documents) > MAX_DOCUMENTS:\n",
        "    documents = documents[:MAX_DOCUMENTS]\n",
        "    print(f\"Limited to {len(documents)} documents to avoid rate limits\")\n",
        "else:\n",
        "    print(f\"Using all {len(documents)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dzQlFFoWlZj1"
      },
      "outputs": [],
      "source": [
        "# split documents\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "documents = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk9-1mcEsPa8"
      },
      "source": [
        "## **Pinecone Vector Database**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCKff2pOritW"
      },
      "outputs": [],
      "source": [
        "# # initialize pinecone client\n",
        "# from pinecone import Pinecone as PineconeClient, ServerlessSpec\n",
        "# pc = PineconeClient(\n",
        "#     api_key=os.environ.get(\"PINECONE_API_KEY\"),\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2do7kkOrsZvR"
      },
      "outputs": [],
      "source": [
        "# # create index\n",
        "# pc.create_index(\n",
        "#         name='my-index',\n",
        "#         dimension=1536,\n",
        "#         metric=\"cosine\",\n",
        "#         spec=ServerlessSpec(\n",
        "#             cloud=\"aws\",\n",
        "#             region=\"us-east-1\"\n",
        "#         )\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzqQvvRpsf4l"
      },
      "outputs": [],
      "source": [
        "# # load index\n",
        "# index_name = \"my-index\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7YSkjF5slji"
      },
      "outputs": [],
      "source": [
        "# # create vectorstore\n",
        "# from langchain.vectorstores import Pinecone\n",
        "# vectorstore = Pinecone.from_documents(\n",
        "#     documents=documents,\n",
        "#     embedding=embeddings,\n",
        "#     index_name=index_name\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En1bq6B0srDA"
      },
      "source": [
        "## **FAISS (Optional)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "l1SQyeIrlcDk"
      },
      "outputs": [],
      "source": [
        "# optional vectorstore\n",
        "#%pip install --q faiss-cpu\n",
        "\n",
        "# create vectorstore\n",
        "from langchain.vectorstores import FAISS\n",
        "vectorstore = FAISS.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 EXPLORING THE FAISS VECTORSTORE\n",
            "==================================================\n",
            "📊 Vectorstore Type: <class 'langchain_community.vectorstores.faiss.FAISS'>\n",
            "📊 Number of vectors in index: 320\n",
            "📊 Vector dimension: 768\n",
            "\n",
            "📄 SAMPLE DOCUMENTS IN VECTORSTORE:\n",
            "----------------------------------------\n",
            "\n",
            "📄 Document 1 (ID: bf2bc494-5673-4587-b6ec-47d6dfac6dd3):\n",
            "   Text preview: context: ['African immigration to the United States refers to immigrants to the United States who are or were nationals of modern African countries. T...\n",
            "   Metadata: {'source': '../data/context.csv', 'row': 0}\n",
            "   Full text length: 500 characters\n",
            "\n",
            "📄 Document 2 (ID: 8c0de99b-0a84-4f72-acd1-77e7d399ef5d):\n",
            "   Text preview: do not constitute a homogeneous group. They include peoples from different national, linguistic, ethnic, racial, cultural and social backgrounds. As s...\n",
            "   Metadata: {'source': '../data/context.csv', 'row': 0}\n",
            "   Full text length: 497 characters\n",
            "\n",
            "📄 Document 3 (ID: 99275b37-9eb5-4f54-91d2-1a337684c022):\n",
            "   Text preview: Immigration legislation ==\\n\\n\\n=== Citizenship ===\\nIn the 1870s, the Naturalization Act was extended to allow \"aliens, being free white persons and ...\n",
            "   Metadata: {'source': '../data/context.csv', 'row': 0}\n",
            "   Full text length: 496 characters\n",
            "\n",
            "📄 Document 4 (ID: 9c936381-8e70-401b-8c2f-930461f7f76b):\n",
            "   Text preview: effect until they were repealed in 1965. While the laws were aimed at restricting the immigration of Jews and Catholics from Southern and Eastern Euro...\n",
            "   Metadata: {'source': '../data/context.csv', 'row': 0}\n",
            "   Full text length: 497 characters\n",
            "\n",
            "📄 Document 5 (ID: 4398b03e-f7a9-44c8-9400-253c61392f24):\n",
            "   Text preview: also known as the Johnson-Reed Act, reduced that to 2% of the number of people from that country who were already living in the U.S. in 1890. Under th...\n",
            "   Metadata: {'source': '../data/context.csv', 'row': 0}\n",
            "   Full text length: 498 characters\n",
            "\n",
            "🔍 TESTING SIMILARITY SEARCH:\n",
            "----------------------------------------\n",
            "Query: 'World War'\n",
            "Found 3 similar documents:\n",
            "\n",
            "📄 Result 1:\n",
            "   Text: 1927, an increase in international lending and borrowing between libraries  following the First World War led to the establishment of the International Federation of Library Associations and Instituti...\n",
            "   Source: ../data/context.csv\n",
            "   Row: 2\n",
            "\n",
            "📄 Result 2:\n",
            "   Text: 515; the Weapons of Mass Destruction Proliferators Sanctions Regulations (\"WMDPSR\"), 31 C.F.R. part 544; Executive Order 13382, \"Blocking Property of Weapons of Mass Destruction Proliferators and Thei...\n",
            "   Source: ../data/context.csv\n",
            "   Row: 1\n",
            "\n",
            "📄 Result 3:\n",
            "   Text: dēor in other dead Germanic languages have the general sense of animal, such as Old High German tior, Old Norse djur or dȳr, Gothic dius, Old Saxon dier, and Old Frisian diar. This general sense gave ...\n",
            "   Source: ../data/context.csv\n",
            "   Row: 12\n",
            "\n",
            "🧠 UNDERSTANDING EMBEDDINGS:\n",
            "----------------------------------------\n",
            "Each document is converted to a 1536-dimensional vector using Azure OpenAI\n",
            "FAISS stores these vectors and enables fast similarity search\n",
            "When you query, your question is also converted to a vector\n",
            "FAISS finds the most similar vectors (closest in high-dimensional space)\n",
            "\n",
            "💡 This is how RAG retrieves relevant context for your questions!\n"
          ]
        }
      ],
      "source": [
        "# Explore the FAISS vectorstore for educational purposes\n",
        "print(\"🔍 EXPLORING THE FAISS VECTORSTORE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Basic vectorstore information\n",
        "print(f\"📊 Vectorstore Type: {type(vectorstore)}\")\n",
        "print(f\"📊 Number of vectors in index: {vectorstore.index.ntotal}\")\n",
        "print(f\"📊 Vector dimension: {vectorstore.index.d}\")\n",
        "\n",
        "# Show some document texts and their metadata\n",
        "print(f\"\\n📄 SAMPLE DOCUMENTS IN VECTORSTORE:\")\n",
        "print(\"-\" * 40)\n",
        "docstore_dict = vectorstore.docstore._dict\n",
        "sample_docs = list(docstore_dict.items())[:5]  # Show first 5 documents\n",
        "\n",
        "for i, (doc_id, doc) in enumerate(sample_docs):\n",
        "    print(f\"\\n📄 Document {i+1} (ID: {doc_id}):\")\n",
        "    print(f\"   Text preview: {doc.page_content[:150]}...\")\n",
        "    print(f\"   Metadata: {doc.metadata}\")\n",
        "    print(f\"   Full text length: {len(doc.page_content)} characters\")\n",
        "\n",
        "# Test similarity search to show how retrieval works\n",
        "print(f\"\\n🔍 TESTING SIMILARITY SEARCH:\")\n",
        "print(\"-\" * 40)\n",
        "test_query = \"World War\"\n",
        "similar_docs = vectorstore.similarity_search(test_query, k=3)\n",
        "\n",
        "print(f\"Query: '{test_query}'\")\n",
        "print(f\"Found {len(similar_docs)} similar documents:\")\n",
        "\n",
        "for i, doc in enumerate(similar_docs):\n",
        "    print(f\"\\n📄 Result {i+1}:\")\n",
        "    print(f\"   Text: {doc.page_content[:200]}...\")\n",
        "    print(f\"   Source: {doc.metadata.get('source', 'Unknown')}\")\n",
        "    print(f\"   Row: {doc.metadata.get('row', 'N/A')}\")\n",
        "\n",
        "# Show how embeddings work conceptually\n",
        "print(f\"\\n🧠 UNDERSTANDING EMBEDDINGS:\")\n",
        "print(\"-\" * 40)\n",
        "print(\"Each document is converted to a 1536-dimensional vector using Azure OpenAI\")\n",
        "print(\"FAISS stores these vectors and enables fast similarity search\")\n",
        "print(\"When you query, your question is also converted to a vector\")\n",
        "print(\"FAISS finds the most similar vectors (closest in high-dimensional space)\")\n",
        "print(\"\\n💡 This is how RAG retrieves relevant context for your questions!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHq5hAhZls9s"
      },
      "source": [
        "## **Retriever**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wazZm3Hzltj0"
      },
      "outputs": [],
      "source": [
        "# create retriever\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwDXLi4elyxP"
      },
      "source": [
        "## **RAG Chain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "KbFmmqZBlzCN"
      },
      "outputs": [],
      "source": [
        "# load llm\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "llm = AzureChatOpenAI(\n",
        "    model=AzureChatOpenAIModel,\n",
        "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
        "    api_key=AZURE_OPENAI_API_KEY,\n",
        "    api_version=AZURE_OPENAI_API_VERSION,\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_ollama.llms import OllamaLLM\n",
        "\n",
        "llm = OllamaLLM(\n",
        "    model=\"deepseek-r1:8b\",  # or another model available in your Ollama instance\n",
        "    base_url=\"http://localhost:11434\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "XTbjjl9Ml4JT"
      },
      "outputs": [],
      "source": [
        "# create document chain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "template = \"\"\"\"\n",
        "You are a helpful assistant that answers questions based on the provided context.\n",
        "Use the provided context to answer the question.\n",
        "Question: {input}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Setup RAG pipeline\n",
        "rag_chain = (\n",
        "    {\"context\": retriever,  \"input\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KCmVRhwkl-L4",
        "outputId": "013b60e8-79f2-4a34-a6f9-025d22a0040e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📄 FIRST DOCUMENT CONTENT:\n",
            "==================================================\n",
            "Content: context: ['African immigration to the United States refers to immigrants to the United States who are or were nationals of modern African countries. The term African in the scope of this article refers to geographical or national origins rather than racial affiliation. Between the Immigration and Nationality Act of 1965 and 2017, Sub-Saharan African-born population in the United States grew to 2.1 million people.Sub-Saharan Africans in the United States come from almost all regions in Africa and\n",
            "Metadata: {'source': '../data/context.csv', 'row': 0}\n",
            "==================================================\n",
            "\n",
            "🤖 RAG Response:\n",
            "<think>\n",
            "Alright, so I need to figure out what African immigration to the United States is based on the provided context. Let's break down each document to understand the key points.\n",
            "\n",
            "First, Document 'aef6c4ab-4ab6-4cd1-86a8-1eac3d40d0db' mentions that African immigration refers to people from modern African countries and that \"African\" here means national origins rather than race. It also notes that between 1965 and 2017, the Sub-Saharan African-born population grew to 2.1 million. This gives a timeframe and a number, which are important metrics.\n",
            "\n",
            "Next, Document 'd6b0a179-6ff6-4f77-967a-f21f373f4f61' talks about migration patterns over time, calling it the \"fourth great migration.\" It states that three-fourths of African out-migration to the U.S. started after 1990. This indicates a significant increase in recent decades.\n",
            "\n",
            "Then, Document 'cb6c8544-6396-4a41-9ebb-cfc4ed9c6bff' seems to repeat some information from the previous document, perhaps confirming that the influx began in the late 20th century and is part of the \"fourth great migration.\"\n",
            "\n",
            "Lastly, Document '46a9ace8-c7aa-4f42-abc7-a8b246550a52' adds more detail by mentioning that preferences led to 45% and 10% of all African immigration in 2016. It also notes the role of legal service organizations like the African Advocacy Network in aiding family sponsorships and mentions refugees from countries like the Central African Republic, South Sudan, Nigeria, and Burundi. However, it's cut off, so I'm not sure if it goes into more detail about refugee numbers or restrictions.\n",
            "\n",
            "Putting this together, African immigration to the U.S. started gaining significant momentum in the latter half of the 20th century, part of a trend known as the \"fourth great migration.\" The number of Sub-Saharan Africans has grown to over 2 million since 1965. In recent years, especially after 1990, a large majority of African immigrants enter through family reunification or refugee programs, with refugees from specific crisis regions being particularly notable.\n",
            "\n",
            "I should also consider that the term \"African\" refers to national origins rather than race, so it's about country of origin rather than skin color. This is important because sometimes terms can be ambiguous, but in this context, it's clarified as geographical or national origins.\n",
            "\n",
            "So, putting all this information together, African immigration to the U.S. has been a significant and growing demographic trend, particularly in the late 20th and early 21st centuries, driven by both legal immigration and refugee programs.\n",
            "</think>\n",
            "\n",
            "**Answer:**\n",
            "\n",
            "African immigration to the United States refers to individuals from modern African countries, focusing on their national origins rather than race. This migration is part of a trend known as the \"fourth great migration,\" which began in the latter half of the 20th century.\n",
            "\n",
            "Key points include:\n",
            "\n",
            "1. **Historical Context**: The influx of African immigrants to the U.S. started gaining significant momentum in the late 20th century, particularly after 1990, with three-fourths of African out-migration to the U.S. occurring post-1990.\n",
            "\n",
            "2. **Demographic Growth**: Between 1965 and 2017, the Sub-Saharan African-born population in the U.S. grew to over 2 million, reflecting a steady increase in immigration.\n",
            "\n",
            "3. **Legal Immigration and Refugee Programs**: A significant portion of African immigration is facilitated through family reunification programs and refugee admissions. In recent years, refugees from countries like the Central African Republic, South Sudan, Nigeria, and Burundi have been particularly notable, though there have been restrictions on refugee entrance to the U.S.\n",
            "\n",
            "4. **Organizational Support**: Legal service organizations, such as the African Advocacy Network, play a crucial role in assisting family members with sponsorship and supporting refugees.\n",
            "\n",
            "In summary, African immigration to the U.S. is a diverse and growing demographic trend, driven by both legal processes and humanitarian concerns, contributing to the cultural and ethnic landscape of the country.\n",
            "\n",
            "🤖 RAG Response:\n",
            "<think>\n",
            "Alright, so I need to figure out what African immigration to the United States is based on the provided context. Let's break down each document to understand the key points.\n",
            "\n",
            "First, Document 'aef6c4ab-4ab6-4cd1-86a8-1eac3d40d0db' mentions that African immigration refers to people from modern African countries and that \"African\" here means national origins rather than race. It also notes that between 1965 and 2017, the Sub-Saharan African-born population grew to 2.1 million. This gives a timeframe and a number, which are important metrics.\n",
            "\n",
            "Next, Document 'd6b0a179-6ff6-4f77-967a-f21f373f4f61' talks about migration patterns over time, calling it the \"fourth great migration.\" It states that three-fourths of African out-migration to the U.S. started after 1990. This indicates a significant increase in recent decades.\n",
            "\n",
            "Then, Document 'cb6c8544-6396-4a41-9ebb-cfc4ed9c6bff' seems to repeat some information from the previous document, perhaps confirming that the influx began in the late 20th century and is part of the \"fourth great migration.\"\n",
            "\n",
            "Lastly, Document '46a9ace8-c7aa-4f42-abc7-a8b246550a52' adds more detail by mentioning that preferences led to 45% and 10% of all African immigration in 2016. It also notes the role of legal service organizations like the African Advocacy Network in aiding family sponsorships and mentions refugees from countries like the Central African Republic, South Sudan, Nigeria, and Burundi. However, it's cut off, so I'm not sure if it goes into more detail about refugee numbers or restrictions.\n",
            "\n",
            "Putting this together, African immigration to the U.S. started gaining significant momentum in the latter half of the 20th century, part of a trend known as the \"fourth great migration.\" The number of Sub-Saharan Africans has grown to over 2 million since 1965. In recent years, especially after 1990, a large majority of African immigrants enter through family reunification or refugee programs, with refugees from specific crisis regions being particularly notable.\n",
            "\n",
            "I should also consider that the term \"African\" refers to national origins rather than race, so it's about country of origin rather than skin color. This is important because sometimes terms can be ambiguous, but in this context, it's clarified as geographical or national origins.\n",
            "\n",
            "So, putting all this information together, African immigration to the U.S. has been a significant and growing demographic trend, particularly in the late 20th and early 21st centuries, driven by both legal immigration and refugee programs.\n",
            "</think>\n",
            "\n",
            "**Answer:**\n",
            "\n",
            "African immigration to the United States refers to individuals from modern African countries, focusing on their national origins rather than race. This migration is part of a trend known as the \"fourth great migration,\" which began in the latter half of the 20th century.\n",
            "\n",
            "Key points include:\n",
            "\n",
            "1. **Historical Context**: The influx of African immigrants to the U.S. started gaining significant momentum in the late 20th century, particularly after 1990, with three-fourths of African out-migration to the U.S. occurring post-1990.\n",
            "\n",
            "2. **Demographic Growth**: Between 1965 and 2017, the Sub-Saharan African-born population in the U.S. grew to over 2 million, reflecting a steady increase in immigration.\n",
            "\n",
            "3. **Legal Immigration and Refugee Programs**: A significant portion of African immigration is facilitated through family reunification programs and refugee admissions. In recent years, refugees from countries like the Central African Republic, South Sudan, Nigeria, and Burundi have been particularly notable, though there have been restrictions on refugee entrance to the U.S.\n",
            "\n",
            "4. **Organizational Support**: Legal service organizations, such as the African Advocacy Network, play a crucial role in assisting family members with sponsorship and supporting refugees.\n",
            "\n",
            "In summary, African immigration to the U.S. is a diverse and growing demographic trend, driven by both legal processes and humanitarian concerns, contributing to the cultural and ethnic landscape of the country.\n"
          ]
        }
      ],
      "source": [
        "# Let's first examine the first document to create a targeted question\n",
        "print(\"📄 FIRST DOCUMENT CONTENT:\")\n",
        "print(\"=\" * 50)\n",
        "first_doc = documents[0]  # Get the first document after splitting\n",
        "print(f\"Content: {first_doc.page_content}\")\n",
        "print(f\"Metadata: {first_doc.metadata}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Based on the first document content, ask a targeted question\n",
        "# This question should retrieve the first document as the most relevant context\n",
        "response = rag_chain.invoke(\"What is African immigration to the United States?\")\n",
        "print(f\"\\n🤖 RAG Response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydmLJTKcmN-L"
      },
      "source": [
        "from langchain_ollama.llms import Ollama\n",
        "\n",
        "llm = Ollama(\n",
        "    model=\"deepseek-r1:8b\",  # or another model available in your Ollama instance\n",
        "    base_url=\"http://localhost:11434\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8mYI56TXmNMo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_64214/3896599713.py:9: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n"
          ]
        }
      ],
      "source": [
        "# create dataset\n",
        "question = [\"What is African immigration to the United States?\"]\n",
        "response = []\n",
        "contexts = []\n",
        "\n",
        "# Inference\n",
        "for query in question:\n",
        "  response.append(rag_chain.invoke(query))\n",
        "  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n",
        "\n",
        "# To dict\n",
        "data = {\n",
        "    \"query\": question,\n",
        "    \"response\": response,\n",
        "    \"context\": contexts,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fsvXVMDZmlYC"
      },
      "outputs": [],
      "source": [
        "# create dataset\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_dict(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "c6ENh8IImn8X"
      },
      "outputs": [],
      "source": [
        "# create dataframe\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "jbBOytBzmp03",
        "outputId": "295f18de-9923-4a49-8c0c-4fa950620a79"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>query</th>\n",
              "      <th>response</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is African immigration to the United States?</td>\n",
              "      <td>African immigration to the United States refer...</td>\n",
              "      <td>[context: ['African immigration to the United ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               query  \\\n",
              "0  What is African immigration to the United States?   \n",
              "\n",
              "                                            response  \\\n",
              "0  African immigration to the United States refer...   \n",
              "\n",
              "                                             context  \n",
              "0  [context: ['African immigration to the United ...  "
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qRQUgNeemx1S"
      },
      "outputs": [],
      "source": [
        "# Convert to dictionary\n",
        "df_dict = df.to_dict(orient='records')\n",
        "\n",
        "# Convert context to list\n",
        "for record in df_dict:\n",
        "    if not isinstance(record.get('context'), list):\n",
        "        if record.get('context') is None:\n",
        "            record['context'] = []\n",
        "        else:\n",
        "            record['context'] = [record['context']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r2bDl3-m0UU"
      },
      "source": [
        "## **Evaluation in Athina AI**\n",
        "\n",
        "We will use **Does Response Answer Query** eval here. It Checks if the response answer the user's query. To learn more about this. Please refer to our [documentation](https://docs.athina.ai/api-reference/evals/preset-evals/overview) for further details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZJLIyV0m1As"
      },
      "outputs": [],
      "source": [
        "# # set api keys for Athina evals\n",
        "# from athina.keys import AthinaApiKey, OpenAiApiKey\n",
        "# OpenAiApiKey.set_key(os.getenv('OPENAI_API_KEY'))\n",
        "# AthinaApiKey.set_key(os.getenv('ATHINA_API_KEY'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hKmTTy9m4mh"
      },
      "outputs": [],
      "source": [
        "# # load dataset\n",
        "# from athina.loaders import Loader\n",
        "# dataset = Loader().load_dict(df_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "_c6tIOXXm6rC",
        "outputId": "9d4af1ae-6a35-4093-9b14-50c0ee13863b"
      },
      "outputs": [],
      "source": [
        "# # evaluate\n",
        "# from athina.evals import DoesResponseAnswerQuery\n",
        "# DoesResponseAnswerQuery(model=\"gpt-4o\").run_batch(data=dataset).to_df()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
