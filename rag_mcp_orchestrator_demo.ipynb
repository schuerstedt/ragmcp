{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c55106",
   "metadata": {},
   "source": [
    "# RAG + MCP Demo - Orchestrator Template\n",
    "\n",
    "This notebook demonstrates using MCP (Model Context Protocol) to orchestrate RAG workers.\n",
    "\n",
    "**Focus:** Practical usage - how to use FastMCP to coordinate multiple RAG approaches.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64de47d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import required libraries\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "# FastMCP for client connection\n",
    "from fastmcp import Client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243f5b28",
   "metadata": {},
   "source": [
    "## Import MCP Server\n",
    "\n",
    "Import the server from `server.py` to reuse the same logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e22f7c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MCP server imported successfully!\n",
      "üì¶ Server: rag-notebook-orchestrator\n"
     ]
    }
   ],
   "source": [
    "# üîß Import and setup MCP server\n",
    "import server\n",
    "import importlib\n",
    "importlib.reload(server)  # Force reload to get updated code\n",
    "\n",
    "# Access the MCP server instance\n",
    "mcp = server.mcp\n",
    "\n",
    "print(\"‚úÖ MCP server imported successfully!\")\n",
    "print(f\"üì¶ Server: {mcp.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d33b3",
   "metadata": {},
   "source": [
    "## Connect MCP Client\n",
    "\n",
    "Connect to the imported MCP server using FastMCP's in-memory transport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b37333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Available MCP tools:\n",
      "====================================================================================================\n",
      "1. üîß Tool: rag_semantic_search\n",
      "   üìù Description: Search documents using semantic similarity matching approach.\n",
      "\n",
      "This tool performs retrieval-augmented generation using semantic similarity\n",
      "to find relevant document chunks. It uses embedding-based matching to \n",
      "understand query intent beyond keyword matching.\n",
      "\n",
      "Args:\n",
      "    query: The search query or question to process. Should be a natural\n",
      "           language question or statement. Examples:\n",
      "           - \"What is machine learning?\"\n",
      "           - \"Explain neural networks\"\n",
      "           - \"How does gradient descent work?\"\n",
      "    return_chunks: Whether to return the retrieved chunks in the response.\n",
      "                  Useful for inspection, debugging, or further processing.\n",
      "    return_answer: Whether to return the generated answer. Can be disabled\n",
      "                  if only chunks are needed for downstream processing.\n",
      "    reindex: Whether to rebuild the search index. Use True to force\n",
      "            reindexing when the corpus has changed.\n",
      "\n",
      "Returns:\n",
      "    dict: The data object from the notebook containing:\n",
      "        - version: Tool version identifier\n",
      "        - metrics: Performance metrics (always included)\n",
      "        - chunks: Retrieved chunks (if return_chunks=True)\n",
      "        - answer: Generated response (if return_answer=True)\n",
      "\n",
      "Example usage:\n",
      "    # Get answer only\n",
      "    result = rag_semantic_search(\"What is deep learning?\")\n",
      "    print(result[\"answer\"])  # Direct access to answer\n",
      "    \n",
      "    # Get chunks only for further processing\n",
      "    result = rag_semantic_search(\"ML concepts\", return_chunks=True, return_answer=False)\n",
      "    chunks = result[\"chunks\"]  # Direct access to chunks\n",
      "    \n",
      "    # Get everything for transparency\n",
      "    result = rag_semantic_search(\"AI overview\", return_chunks=True, return_answer=True)\n",
      "    print(result[\"version\"], result[\"metrics\"])  # Direct access to all fields\n",
      "   ‚öôÔ∏è  Input Schema: {'properties': {'query': {'title': 'Query', 'type': 'string'}, 'return_chunks': {'default': False, 'title': 'Return Chunks', 'type': 'boolean'}, 'return_answer': {'default': True, 'title': 'Return Answer', 'type': 'boolean'}, 'reindex': {'default': False, 'title': 'Reindex', 'type': 'boolean'}}, 'required': ['query'], 'type': 'object'}\n",
      "--------------------------------------------------\n",
      "2. üîß Tool: rag_enhanced_search\n",
      "   üìù Description: Advanced search with enhanced processing and detailed analytics.\n",
      "\n",
      "This tool provides more sophisticated retrieval-augmented generation\n",
      "with enhanced processing capabilities, chunk analysis, and detailed\n",
      "performance metrics. Suitable for complex queries requiring deeper analysis.\n",
      "\n",
      "Args:\n",
      "    query: The search query or question to process. This tool handles\n",
      "           complex, multi-part questions better than basic semantic search.\n",
      "           Examples:\n",
      "           - \"Compare supervised vs unsupervised learning\"\n",
      "           - \"Explain the relationship between AI, ML, and deep learning\"\n",
      "           - \"What are the pros and cons of different optimization algorithms?\"\n",
      "    return_chunks: Whether to return the retrieved chunks in the response.\n",
      "                  Useful for chunk analysis, reranking, or combination with other tools.\n",
      "    return_answer: Whether to return the generated answer. Can be disabled\n",
      "                  if only enhanced chunk retrieval is needed.\n",
      "    reindex: Whether to rebuild the search index. Use True to force\n",
      "            reindexing when the corpus has changed.\n",
      "\n",
      "Returns:\n",
      "    dict: The data object from the notebook containing:\n",
      "        - version: Tool version identifier\n",
      "        - metrics: Extended performance metrics (always included)\n",
      "        - chunks: Enhanced chunks with metadata (if return_chunks=True)\n",
      "        - answer: Generated response (if return_answer=True)\n",
      "        - capabilities: Tool capabilities for introspection\n",
      "\n",
      "Features:\n",
      "    - Enhanced confidence scoring\n",
      "    - Chunk analysis and reporting\n",
      "    - Advanced processing methods\n",
      "    - Extended capability reporting\n",
      "\n",
      "Example usage:\n",
      "    # Get enhanced answer\n",
      "    result = rag_enhanced_search(\"How do transformers work in NLP?\")\n",
      "    print(result[\"answer\"])  # Direct access\n",
      "    \n",
      "    # Get enhanced chunks for reranking\n",
      "    result = rag_enhanced_search(\"AI concepts\", return_chunks=True, return_answer=False)\n",
      "    chunks = result[\"chunks\"]  # Enhanced chunks with metadata\n",
      "    \n",
      "    # Get full analysis\n",
      "    result = rag_enhanced_search(\"ML overview\", return_chunks=True, return_answer=True)\n",
      "    print(result[\"capabilities\"])  # Tool capabilities\n",
      "   ‚öôÔ∏è  Input Schema: {'properties': {'query': {'title': 'Query', 'type': 'string'}, 'return_chunks': {'default': False, 'title': 'Return Chunks', 'type': 'boolean'}, 'return_answer': {'default': True, 'title': 'Return Answer', 'type': 'boolean'}, 'reindex': {'default': False, 'title': 'Reindex', 'type': 'boolean'}}, 'required': ['query'], 'type': 'object'}\n",
      "--------------------------------------------------\n",
      "3. üîß Tool: list_workers\n",
      "   üìù Description: List all available worker notebooks for debugging and introspection.\n",
      "\n",
      "This development utility lists the underlying notebook files that power\n",
      "the RAG tools. Useful for debugging, development, and understanding\n",
      "the system architecture.\n",
      "\n",
      "Returns:\n",
      "    list[str]: List of notebook filenames in the workers directory.\n",
      "              Each filename corresponds to a specific RAG implementation.\n",
      "\n",
      "Note:\n",
      "    This is a development/debugging tool. In production scenarios,\n",
      "    clients should use the specific RAG tools (rag_semantic_search,\n",
      "    rag_enhanced_search) rather than working with raw notebook files.\n",
      "\n",
      "Example usage:\n",
      "    workers = list_workers()\n",
      "    print(f\"Available workers: {workers}\")\n",
      "   ‚öôÔ∏è  Input Schema: {'properties': {}, 'type': 'object'}\n",
      "--------------------------------------------------\n",
      "====================================================================================================\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# üìã List available tools using async context\n",
    "async with Client(mcp) as client:\n",
    "    # List tools\n",
    "    tools = await client.list_tools()\n",
    "    print(\"üì¶ Available MCP tools:\")\n",
    "    print(\"=\" * 100)  # Visual separator\n",
    "    for i, tool in enumerate(tools, 1):\n",
    "        print(f\"{i}. üîß Tool: {tool.name}\")\n",
    "        print(f\"   üìù Description: {tool.description}\")\n",
    "        print(f\"   ‚öôÔ∏è  Input Schema: {tool.inputSchema}\")\n",
    "        print(\"-\" * 50)\n",
    "    print(\"=\" * 100)  # Visual separator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c224beee",
   "metadata": {},
   "source": [
    "## Run RAG Demo\n",
    "\n",
    "Execute multiple RAG workers through the MCP interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56956610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running RAG demo with query: 'What is machine learning?'\n",
      "üí° Testing both specific RAG tools:\n",
      "\n",
      "üîÑ Testing rag_semantic_search...\n",
      "üîß DEBUG: Executing rag_hello_one_mcp.ipynb with parameters: {'query': 'What is machine learning?', 'return_chunks': False, 'return_answer': True, 'reindex': False}\n",
      "üìÅ DEBUG: Server dir: f:\\LernRAG\\ragmcp\n",
      "üìÅ DEBUG: Input notebook: f:\\LernRAG\\ragmcp\\workers\\rag_hello_one_mcp.ipynb\n",
      "üìÅ DEBUG: Output notebook: f:\\LernRAG\\ragmcp\\runs\\rag_hello_one_mcp_out.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1f695683c74f93a1ebef6fd2b214ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/7 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DEBUG: Extracted data: {'answer': \"MCP-ONE: This is a generated answer for 'What is machine learning?' based on retrieved context.\", 'metrics': {'chunks_retrieved': 3, 'confidence': 0.85, 'method': 'semantic_search_demo', 'processing_time': 0.12, 'query_length': 25}, 'version': 'one@mcp'}\n",
      "üíæ DEBUG: Executed notebook saved to: f:\\LernRAG\\ragmcp\\runs\\rag_hello_one_mcp_out.ipynb\n",
      "‚úÖ rag_semantic_search completed successfully\n",
      "üìù Answer: MCP-ONE: This is a generated answer for 'What is machine learning?' based on retrieved context.\n",
      "üìà Metrics: {'chunks_retrieved': 3, 'confidence': 0.85, 'method': 'semantic_search_demo', 'processing_time': 0.12, 'query_length': 25}\n",
      "üè∑Ô∏è  Version: one@mcp\n",
      "\n",
      "üîÑ Testing rag_enhanced_search...\n",
      "üîß DEBUG: Executing rag_hello_two_mcp.ipynb with parameters: {'query': 'What is machine learning?', 'return_chunks': False, 'return_answer': True, 'reindex': False}\n",
      "üìÅ DEBUG: Server dir: f:\\LernRAG\\ragmcp\n",
      "üìÅ DEBUG: Input notebook: f:\\LernRAG\\ragmcp\\workers\\rag_hello_two_mcp.ipynb\n",
      "üìÅ DEBUG: Output notebook: f:\\LernRAG\\ragmcp\\runs\\rag_hello_two_mcp_out.ipynb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e6818510b544b7b9f58a1259f16eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/7 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DEBUG: Extracted data: {'answer': \"MCP-TWO: Enhanced analysis for 'What is machine learning?' using advanced semantic processing and cross-referencing.\", 'capabilities': ['semantic_search', 'metadata_analysis', 'cross_referencing'], 'metrics': {'chunks_retrieved': 3, 'confidence': 0.92, 'method': 'enhanced_semantic_search', 'processing_time': 0.08, 'query_length': 25}, 'version': 'two@mcp'}\n",
      "üíæ DEBUG: Executed notebook saved to: f:\\LernRAG\\ragmcp\\runs\\rag_hello_two_mcp_out.ipynb\n",
      "‚úÖ rag_enhanced_search completed successfully\n",
      "üìù Answer: MCP-TWO: Enhanced analysis for 'What is machine learning?' using advanced semantic processing and cross-referencing.\n",
      "üìà Metrics: {'chunks_retrieved': 3, 'confidence': 0.92, 'method': 'enhanced_semantic_search', 'processing_time': 0.08, 'query_length': 25}\n",
      "üè∑Ô∏è  Version: two@mcp\n",
      "\n",
      "‚ú® MCP RAG tool demonstration complete!\n"
     ]
    }
   ],
   "source": [
    "# Demo: Execute specific RAG tools through MCP interface\n",
    "async with Client(mcp) as client:\n",
    "    \n",
    "    # Test parameters for RAG query\n",
    "    test_query = \"What is machine learning?\"\n",
    "    \n",
    "    print(f\"üöÄ Running RAG demo with query: '{test_query}'\")\n",
    "    print(\"üí° Testing both specific RAG tools:\")\n",
    "    \n",
    "    # Test semantic search tool\n",
    "    print(f\"\\nüîÑ Testing rag_semantic_search...\")\n",
    "    try:\n",
    "        result = await client.call_tool(\"rag_semantic_search\", {\n",
    "            \"query\": test_query\n",
    "        })\n",
    "        \n",
    "        # Extract results using correct data access\n",
    "        worker_data = result.data\n",
    "        \n",
    "        if worker_data:\n",
    "            print(f\"‚úÖ rag_semantic_search completed successfully\")\n",
    "            print(f\"üìù Answer: {worker_data.get('answer', 'N/A')}\")\n",
    "            if \"metrics\" in worker_data:\n",
    "                print(f\"üìà Metrics: {worker_data['metrics']}\")\n",
    "            if \"version\" in worker_data:\n",
    "                print(f\"üè∑Ô∏è  Version: {worker_data['version']}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  rag_semantic_search completed but returned no data\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå rag_semantic_search failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Test enhanced search tool\n",
    "    print(f\"\\nüîÑ Testing rag_enhanced_search...\")\n",
    "    try:\n",
    "        result = await client.call_tool(\"rag_enhanced_search\", {\n",
    "            \"query\": test_query\n",
    "        })\n",
    "        \n",
    "        # Extract results using correct data access\n",
    "        worker_data = result.data\n",
    "        \n",
    "        if worker_data:\n",
    "            print(f\"‚úÖ rag_enhanced_search completed successfully\")\n",
    "            print(f\"üìù Answer: {worker_data.get('answer', 'N/A')}\")\n",
    "            if \"metrics\" in worker_data:\n",
    "                print(f\"üìà Metrics: {worker_data['metrics']}\")\n",
    "            if \"version\" in worker_data:\n",
    "                print(f\"üè∑Ô∏è  Version: {worker_data['version']}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  rag_enhanced_search completed but returned no data\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå rag_enhanced_search failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "print(\"\\n‚ú® MCP RAG tool demonstration complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
