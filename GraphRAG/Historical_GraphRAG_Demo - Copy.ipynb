{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe813b8",
   "metadata": {},
   "source": [
    "# üìö Learning Path: Graphs, LLMs, and Knowledge Extraction with Historical Data\n",
    "Welcome to this hands-on notebook! This guided journey is designed for beginners in both knowledge graphs and large language models (LLMs). By the end, you will:\n",
    "- Understand what a knowledge graph is and why it matters for reasoning over data.\n",
    "- Learn how to use LLMs to extract structured knowledge from unstructured text.\n",
    "- See how to build, query, and visualize a knowledge graph from historical data.\n",
    "- Practice asking questions and interpreting answers using both graph and LLM reasoning.\n",
    "- Reflect on each step with learner prompts to deepen your understanding.\n",
    "\n",
    "**How to use this notebook:**\n",
    "- Read the explanations and learner comments in each cell.\n",
    "- Run the code cells in order, and observe the outputs.\n",
    "- After each section, pause to consider the learner prompts and questions.\n",
    "- Try modifying the code or data to experiment and learn actively!\n",
    "\n",
    "Let's begin your journey into the world of graphs and LLMs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756064f",
   "metadata": {},
   "source": [
    "# üß† Historical GraphRAG Notebook\n",
    "Welcome! In this notebook, you'll learn how to combine knowledge graphs and large language models (LLMs) to extract, organize, and reason about information from historical texts.\n",
    "\n",
    "**What is a Knowledge Graph?**\n",
    "A knowledge graph is a way to represent information as a network of entities (like people, places, or events) and the relationships between them. This makes it easier to explore connections and answer complex questions.\n",
    "\n",
    "**Why use LLMs?**\n",
    "LLMs can help us extract structured knowledge (facts and relationships) from unstructured text, making it possible to build and update knowledge graphs automatically.\n",
    "\n",
    "As you go through each step, look for the learner prompts and questions to help you reflect and deepen your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757bf1b3",
   "metadata": {},
   "source": [
    "## üîß Environment Setup\n",
    "Before we start, let's make sure we have all the necessary tools. We'll use several Python libraries:\n",
    "- `langchain` and `langchain-core`: For working with LLMs and building chains of reasoning.\n",
    "- `pandas`: For handling tabular data.\n",
    "- `networkx`: For building and visualizing graphs.\n",
    "- `dotenv`: For securely loading API keys and settings.\n",
    "\n",
    "**Learner Prompt:**\n",
    "- Why do you think we need both a graph library and an LLM library? What might each be responsible for in this project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106bb4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries. This ensures you have the latest versions for working with LLMs and graphs.\n",
    "# You only need to run this once per environment.\n",
    "%pip pip install -r requirements.txt\n",
    "\n",
    "# Learner Prompt:\n",
    "# - What do you think would happen if you skipped this step or used older versions of these libraries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc4d32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries we need for this project.\n",
    "import os  # For interacting with the operating system and environment variables\n",
    "import pandas as pd  # For loading and manipulating tabular data\n",
    "import networkx as nx  # For creating and working with graphs\n",
    "from collections import defaultdict  # For counting and grouping data efficiently\n",
    "from langchain.chat_models import AzureChatOpenAI  # For using Azure's LLM\n",
    "from langchain.prompts import PromptTemplate  # For creating prompts for the LLM\n",
    "from langchain_core.output_parsers import StrOutputParser  # For parsing LLM outputs\n",
    "from langchain_community.document_loaders import CSVLoader  # For loading CSV files as documents\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # For splitting text into manageable chunks\n",
    "from langchain.schema import Document  # For working with document objects\n",
    "from dotenv import load_dotenv  # For loading environment variables from a .env file\n",
    "\n",
    "# Load environment variables (like API keys) from a .env file for security.\n",
    "load_dotenv()\n",
    "\n",
    "# Learner Prompt:\n",
    "# - Can you identify which libraries are for graphs, which are for LLMs, and which are for data handling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d29038",
   "metadata": {},
   "source": [
    "## ü§ñ Azure OpenAI Setup\n",
    "To use a powerful LLM, we need to connect to Azure OpenAI. This requires API keys and endpoint information, which are kept secret for security.\n",
    "\n",
    "**Learner Prompt:**\n",
    "- Why do you think it's important to keep API keys and endpoints out of your code? How does using a `.env` file help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e019ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure OpenAI credentials from the .env file.\n",
    "# This keeps sensitive information (like API keys) out of your code.\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Get the parent directory of the current working directory (where the notebook is running)\n",
    "parent_dir = Path.cwd().parent\n",
    "env_path = parent_dir / \".env\"\n",
    "\n",
    "# Load the .env file from the parent directory\n",
    "load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "AzureOpenAIEmbeddingsModel = os.getenv(\"AzureOpenAIEmbeddingsModel\", \"text-embedding-ada-002\")\n",
    "AzureChatOpenAIModel = os.getenv(\"AzureChatOpenAIModel\")\n",
    "\n",
    "# Learner Prompt:\n",
    "# - What could go wrong if you accidentally share your .env file or hard-code your API key in a public notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a06fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Azure OpenAI LLM (Large Language Model) using the credentials above.\n",
    "# This model will help us extract knowledge from text and answer questions.\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    model=AzureChatOpenAIModel,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,    \n",
    "    temperature=1\n",
    "# Temperature controls randomness: higher = more creative, lower = more focused\n",
    ")\n",
    "\n",
    "# Learner Prompt:\n",
    "# - What do you think happens if you set the temperature to 0? To 2? Try changing it and see what kind of responses you get."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edff07d",
   "metadata": {},
   "source": [
    "## üìÑ Load and Chunk Historical Text\n",
    "To build a knowledge graph, we first need to load our historical data. We'll use a CSV file with information about historical figures. Because LLMs work best with short pieces of text, we'll split the data into manageable chunks.\n",
    "\n",
    "**Learner Prompt:**\n",
    "- Why do you think it's important to split long documents into smaller chunks before processing them with an LLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the historical data from a CSV file as a list of documents.\n",
    "loader = CSVLoader(file_path=\"historical_figures.csv\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the documents into smaller chunks for easier processing by the LLM.\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = splitter.split_documents(docs)\n",
    "\n",
    "# Learner Prompt:\n",
    "# - What might happen if you use a very large chunk size? What about a very small one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ef536e",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Entity Extraction Prompt (with Example)\n",
    "Now we'll use the LLM to extract structured knowledge from our text. We'll ask it to find entities (like people or events) and the relationships between them, and return these as triples (Subject, Predicate, Object).\n",
    "This is a key step in building a knowledge graph from unstructured data.\n",
    "\n",
    "**Learner Prompt:**\n",
    "- Why do you think we use the format (Subject, Predicate, Object) for representing knowledge? Can you think of an example from your own experience?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d7d62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template for the LLM to extract triples and new entities from text.\n",
    "dynamic_entity_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert in information extraction. Analyze the following text and extract factual triples and new entities.\n",
    "\n",
    "Extract factual triples and new entities from the following text.\n",
    "\n",
    "- Return the triples as plain lines in the format: Subject, Predicate, Object\n",
    "- Do not use dashes, bullets, or numbering.\n",
    "- Avoid extra punctuation like periods at the end.\n",
    "- Return the new entities as a plain list, one per line.\n",
    "- Do not include the triples in the entity list.\n",
    "\n",
    "Text:\n",
    "{input}\n",
    "\n",
    "Known Entities:\n",
    "{known}\n",
    "\n",
    "Respond in this format exactly:\n",
    "\n",
    "Triples:\n",
    "Subject1, Predicate1, Object1\n",
    "Subject2, Predicate2, Object2\n",
    "\n",
    "New Entities:\n",
    "Entity1\n",
    "Entity2\n",
    "Entity3\n",
    "\"\"\")\n",
    "dynamic_entity_chain = dynamic_entity_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Learner Prompt:\n",
    "# - Why is it important to be very specific about the output format when working with LLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54eaa19",
   "metadata": {},
   "source": [
    "## üåê Build Knowledge Graph from Triples\n",
    "Once we have extracted triples (facts) from the text, we can build a knowledge graph. Each triple becomes a connection (edge) between two entities (nodes) in the graph.\n",
    "This graph structure allows us to explore relationships and answer questions that would be hard to solve with plain text.\n",
    "\n",
    "**Learner Prompt:**\n",
    "- How might a knowledge graph help you find connections between historical figures that aren't obvious from reading text alone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e38d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the knowledge graph from the extracted triples.\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from networkx.readwrite import json_graph\n",
    "\n",
    "# === CONFIG ===\n",
    "REINDEX = True  # Set to False to skip LLM re-indexing if data exists\n",
    "GRAPH_PATH = \"historical_graph.json\"\n",
    "ENTITIES_PATH = \"known_entities.json\"\n",
    "\n",
    "# === Setup containers ===\n",
    "G = nx.MultiDiGraph()\n",
    "all_triples = []\n",
    "entity_usage = defaultdict(int)\n",
    "known_entities = set()\n",
    "MAX_ENTITIES_FOR_PROMPT = 40\n",
    "\n",
    "if not REINDEX and os.path.exists(GRAPH_PATH) and os.path.exists(ENTITIES_PATH):\n",
    "    print(\"üîÅ Loading graph and entities from disk...\")\n",
    "    with open(GRAPH_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        G = json_graph.node_link_graph(data, directed=True, multigraph=True)\n",
    "    with open(ENTITIES_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        known_entities = set(json.load(f))\n",
    "    print(f\"‚úÖ Loaded graph with {len(G.nodes)} nodes and {len(G.edges)} edges.\")\n",
    "else:\n",
    "    print(\"‚öôÔ∏è Rebuilding graph from documents using LLM...\")\n",
    "    for i, doc in enumerate(documents):\n",
    "        start = time.time()\n",
    "\n",
    "        # Sort known entities by usage to prioritize important ones for the prompt.\n",
    "        sorted_entities = sorted(known_entities, key=lambda e: -entity_usage[e])\n",
    "        limited_entities = sorted_entities[:MAX_ENTITIES_FOR_PROMPT]\n",
    "        known_str = \", \".join(limited_entities) if limited_entities else \"(none)\"\n",
    "\n",
    "        # Use the LLM to extract triples and new entities from each document chunk.\n",
    "        output = dynamic_entity_chain.invoke({\"input\": doc.page_content, \"known\": known_str})\n",
    "        print(f\"\\n--- LLM Output for doc {i} ---\\n{output.strip()}\")\n",
    "\n",
    "        # Split the output into triples and new entities.\n",
    "        sections = output.strip().split(\"New Entities:\")\n",
    "        triples_block = sections[0].replace(\"Triples:\", \"\").strip()\n",
    "        new_entities_block = sections[1].strip() if len(sections) > 1 else \"\"\n",
    "\n",
    "        triples = []\n",
    "        for line in triples_block.split(\"\\n\"):\n",
    "            if line.strip():\n",
    "                parts = [p.strip(\" ()\\n\") for p in line.split(\",\")]\n",
    "                if len(parts) == 3:\n",
    "                    triples.append(tuple(parts))\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Skipping malformed triple line: {line}\")\n",
    "\n",
    "        # Normalize new entities by stripping leading/trailing spaces and quotes\n",
    "        new_entities = [e.lstrip(\"- \").strip(\" \\\"'\\n\") for e in new_entities_block.split(\"\\n\") if e.strip()]\n",
    "\n",
    "        all_triples.extend(triples)\n",
    "        known_entities.update(new_entities)\n",
    "\n",
    "        for s, r, o in triples:\n",
    "            # Normalize subject and object by stripping leading/trailing spaces and hyphens\n",
    "            s = s.lstrip(\"- \").strip()\n",
    "            r = r.strip(\" .-\").lower()\n",
    "            o = o.lstrip(\"- \").strip()\n",
    "            G.add_node(s)\n",
    "            G.add_node(o)\n",
    "            G.add_edge(s, o, relation=r)\n",
    "            entity_usage[s] += 1\n",
    "            entity_usage[o] += 1\n",
    "\n",
    "        print(f\"‚è±Ô∏è Processed doc {i} in {round(time.time() - start, 2)}s\")\n",
    "\n",
    "    # === Save graph and entity index ===\n",
    "    print(\"üíæ Saving graph and entities to disk...\")\n",
    "    with open(GRAPH_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_graph.node_link_data(G), f, ensure_ascii=False, indent=2)\n",
    "    with open(ENTITIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sorted(list(known_entities)), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Graph built with {len(G.nodes)} nodes, {len(G.edges)} edges, and {len(known_entities)} tracked entities.\")\n",
    "\n",
    "# Learner Prompt:\n",
    "# - What are some advantages of storing the graph and entities to disk? When might you want to rebuild the graph from scratch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa2c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search the graph for all nodes and edges within a certain depth from an anchor entity.\n",
    "def search_graph(anchor_entity: str, depth: int = 2) -> nx.MultiDiGraph:\n",
    "    if anchor_entity not in G:\n",
    "        raise ValueError(f\"Entity '{anchor_entity}' not found in the graph.\")\n",
    "\n",
    "    # Use BFS to find all nodes within the specified depth\n",
    "    bfs_edges = nx.bfs_edges(G, source=anchor_entity, depth_limit=depth)\n",
    "    nodes_in_scope = {anchor_entity}\n",
    "    edges_in_scope = []\n",
    "\n",
    "    for u, v in bfs_edges:\n",
    "        nodes_in_scope.update([u, v])\n",
    "        edges_in_scope.append((u, v))\n",
    "\n",
    "    # Create a new subgraph from the collected nodes and edges\n",
    "    subgraph = nx.MultiDiGraph()\n",
    "    for u, v in edges_in_scope:\n",
    "        for key in G[u][v]:\n",
    "            relation = G[u][v][key].get(\"relation\", \"\")\n",
    "            subgraph.add_edge(u, v, relation=relation)\n",
    "            subgraph.add_node(u)\n",
    "            subgraph.add_node(v)\n",
    "\n",
    "    return subgraph\n",
    "\n",
    "# Learner Prompt:\n",
    "# - Why might you want to limit the depth of your search in a large graph? What could happen if you set the depth too high?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e26724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative function: search the graph in an undirected way (ignoring edge direction).\n",
    "def search_graph(anchor: str, depth: int = 2) -> nx.Graph:\n",
    "    if anchor not in G:\n",
    "        raise ValueError(f\"Entity '{anchor}' not found in the graph.\")\n",
    "\n",
    "    undirected_G = G.to_undirected()\n",
    "    visited_nodes = set([anchor])\n",
    "    current_level = set([anchor])\n",
    "\n",
    "    for _ in range(depth):\n",
    "        next_level = set()\n",
    "        for node in current_level:\n",
    "            neighbors = undirected_G.neighbors(node)\n",
    "            for neighbor in neighbors:\n",
    "                if neighbor not in visited_nodes:\n",
    "                    next_level.add(neighbor)\n",
    "        visited_nodes.update(next_level)\n",
    "        current_level = next_level\n",
    "\n",
    "    subgraph = G.subgraph(visited_nodes).copy()\n",
    "    return subgraph\n",
    "\n",
    "# Learner Prompt:\n",
    "# - What is the difference between a directed and undirected graph? When might you want to ignore edge direction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd55f8",
   "metadata": {},
   "source": [
    "## üìå Anchor Detection from Question\n",
    "When you ask a question, we need to figure out which entity in the graph is most relevant (the \"anchor\"). We'll use the LLM to match your question to the best entity.\n",
    "\n",
    "**Learner Prompt:**\n",
    "- Why do you think it's helpful to identify an anchor entity before searching the graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e53d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect the main entity (anchor) in a user's question using the LLM.\n",
    "def detect_anchor_entity(question: str) -> str:\n",
    "    MAX_ENTITIES_FOR_ANCHOR_PROMPT = 40\n",
    "\n",
    "    # Use global variables for known_entities and entity_usage\n",
    "    sorted_entities = sorted(known_entities, key=lambda e: -entity_usage.get(e, 0))\n",
    "    limited_entities = sorted_entities[:MAX_ENTITIES_FOR_ANCHOR_PROMPT]\n",
    "    known_entities_list = \"\\n\".join(f\"- {e}\" for e in limited_entities) if limited_entities else \"(none)\"\n",
    "\n",
    "    detect_prompt = PromptTemplate.from_template(f\"\"\"\n",
    "You are a semantic matcher. Your task is to identify the main entity (person, event, or discovery) from the list below that the question is primarily about.\n",
    "\n",
    "Only return the exact entity name from the list. Do not explain.\n",
    "\n",
    "Known Entities:\n",
    "{known_entities_list}\n",
    "\n",
    "Question: {{question}}\n",
    "\"\"\")\n",
    "\n",
    "    detect_chain = detect_prompt | llm | StrOutputParser()\n",
    "    return detect_chain.invoke({\"question\": question}).strip()\n",
    "\n",
    "# Learner Prompt:\n",
    "# - What challenges might an LLM face when trying to match a question to the right entity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10532525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the subgraph around the detected anchor entity to see its connections.\n",
    "def visualize_subgraph(query: str, depth: int = 2):\n",
    "    anchor_entity = detect_anchor_entity(query)\n",
    "    print(f\"üîç Detected anchor entity: {anchor_entity}\")\n",
    "\n",
    "    if anchor_entity not in G:\n",
    "        print(f\"‚ö†Ô∏è Entity '{anchor_entity}' not found in the graph.\")\n",
    "        return\n",
    "\n",
    "    # Extract the subgraph based on depth\n",
    "    visited = set()\n",
    "    queue = [(anchor_entity, 0)]\n",
    "    sub_nodes = set()\n",
    "\n",
    "    while queue:\n",
    "        current_node, current_depth = queue.pop(0)\n",
    "        if current_depth > depth or current_node in visited:\n",
    "            continue\n",
    "        visited.add(current_node)\n",
    "        sub_nodes.add(current_node)\n",
    "        neighbors = list(G.successors(current_node)) + list(G.predecessors(current_node))\n",
    "        for neighbor in neighbors:\n",
    "            queue.append((neighbor, current_depth + 1))\n",
    "            sub_nodes.add(neighbor)\n",
    "\n",
    "    SG = G.subgraph(sub_nodes)\n",
    "\n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    pos = nx.spring_layout(SG, seed=42)\n",
    "    nx.draw_networkx_nodes(SG, pos, node_size=600, node_color=\"#FFEEEE\")\n",
    "    nx.draw_networkx_edges(SG, pos, arrows=True, arrowstyle='-|>', edge_color=\"#666\")\n",
    "    nx.draw_networkx_labels(SG, pos, font_size=10)\n",
    "    edge_labels = {(u, v): data[\"relation\"] for u, v, data in SG.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(SG, pos, edge_labels=edge_labels, font_size=9)\n",
    "    plt.title(f\"Subgraph for: '{anchor_entity}'\", fontsize=14)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Learner Prompt:\n",
    "# - How does visualizing a subgraph help you understand the relationships between entities?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b33879",
   "metadata": {},
   "source": [
    "## ü§î Ask a Question Using the Graph\n",
    "Now you can ask questions about historical figures or events. The system will use the knowledge graph to find relevant facts and help the LLM answer your question.\n",
    "\n",
    "**Learner Prompt:**\n",
    "- How might the answers you get from a knowledge graph differ from those you get from a search engine or a plain LLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74468c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the knowledge graph to answer a question by extracting relevant facts and passing them to the LLM.\n",
    "def answer_question_with_graph(query: str, depth: int = 2):\n",
    "    anchor_entity = detect_anchor_entity(query)\n",
    "    print(f\"üîç Detected anchor entity: {anchor_entity}\")\n",
    "\n",
    "    subgraph = search_graph(anchor_entity, depth=depth)\n",
    "    context = [f\"{u} {data['relation']} {v}.\" for u, v, data in subgraph.edges(data=True)]\n",
    "    context_text = \"\\n\".join(context)\n",
    "\n",
    "    response = llm.invoke(f\"Answer the question based on these facts:\\n\\n{context_text}\\n\\nQuestion: {query}\")\n",
    "\n",
    "    return context, response\n",
    "\n",
    "# Learner Prompt:\n",
    "# - What are the advantages of providing the LLM with a focused set of facts from the graph, instead of the entire dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888dfb88",
   "metadata": {},
   "source": [
    "## üöÄ Try It Out\n",
    "Now it's your turn! Try asking your own questions about the historical data. Experiment with different questions and see how the graph and LLM work together to provide answers.\n",
    "\n",
    "**Learner Prompt:**\n",
    "- What kinds of questions work best with this system? What are its limitations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f13422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Ask a question about a historical figure's contribution.\n",
    "question = \"How did Albert Einstein contribute to the development of the atomic bomb?\"\n",
    "# Set the depth for the subgraph search\n",
    "depth= 2\n",
    "retrieved_chunks, response = answer_question_with_graph(question, depth)\n",
    "\n",
    "print(\"\\nRetrieved Chunks:\")\n",
    "for line in retrieved_chunks:\n",
    "    print(\"-\", line)\n",
    "\n",
    "print(\"\\nAnswer:\")\n",
    "print(response)\n",
    "\n",
    "visualize_subgraph(question, depth)\n",
    "\n",
    "# Learner Prompt:\n",
    "# - Try changing the question or the depth. What do you notice about the retrieved facts and the answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa80f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all nodes (entities) in the knowledge graph.\n",
    "print(list(G.nodes))\n",
    "\n",
    "# Learner Prompt:\n",
    "# - How many entities are in your graph? Are there any you didn't expect to see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7798e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all edges related to 'relativity' to explore specific relationships in the graph.\n",
    "for u, v, data in G.edges(data=True):\n",
    "    if \"relativity\" in u.lower() or \"relativity\" in v.lower():\n",
    "        print(f\"{u} --{data['relation']}--> {v}\")\n",
    "\n",
    "# Learner Prompt:\n",
    "# - Try searching for other keywords or relationships. What patterns or connections do you discover?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
