{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f756064f",
   "metadata": {},
   "source": [
    "# ðŸ§  Historical GraphRAG Notebook\n",
    "This notebook demonstrates how to use a lightweight knowledge graph with LangChain + Azure OpenAI to extract and reason over relationships between historical entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757bf1b3",
   "metadata": {},
   "source": [
    "## ðŸ”§ Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106bb4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade langchain langchain-core\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc4d32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser  # âœ… CORRECT\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d29038",
   "metadata": {},
   "source": [
    "## ðŸ¤– Azure OpenAI Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e019ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Get the parent directory of the current working directory (where the notebook is running)\n",
    "parent_dir = Path.cwd().parent\n",
    "env_path = parent_dir / \".env\"\n",
    "\n",
    "# Load the .env file from the parent directory\n",
    "load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "AzureOpenAIEmbeddingsModel = os.getenv(\"AzureOpenAIEmbeddingsModel\", \"text-embedding-ada-002\")\n",
    "AzureChatOpenAIModel = os.getenv(\"AzureChatOpenAIModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a06fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load llm\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    model=AzureChatOpenAIModel,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,    \n",
    "    temperature=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edff07d",
   "metadata": {},
   "source": [
    "## ðŸ“„ Load and Chunk Historical Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=\"historical_figures.csv\")\n",
    "docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ef536e",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Entity Extraction Prompt (with Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d7d62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_entity_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert in information extraction. Analyze the following text and extract factual triples and new entities.\n",
    "\n",
    "Extract factual triples and new entities from the following text.\n",
    "\n",
    "- Return the triples as plain lines in the format: Subject, Predicate, Object\n",
    "- Do not use dashes, bullets, or numbering.\n",
    "- Avoid extra punctuation like periods at the end.\n",
    "- Return the new entities as a plain list, one per line.\n",
    "- Do not include the triples in the entity list.\n",
    "\n",
    "Text:\n",
    "{input}\n",
    "\n",
    "Known Entities:\n",
    "{known}\n",
    "\n",
    "Respond in this format exactly:\n",
    "\n",
    "Triples:\n",
    "Subject1, Predicate1, Object1\n",
    "Subject2, Predicate2, Object2\n",
    "\n",
    "New Entities:\n",
    "Entity1\n",
    "Entity2\n",
    "Entity3\n",
    "\"\"\")\n",
    "dynamic_entity_chain = dynamic_entity_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54eaa19",
   "metadata": {},
   "source": [
    "## ðŸŒ Build Knowledge Graph from Triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e38d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from networkx.readwrite import json_graph\n",
    "\n",
    "# === CONFIG ===\n",
    "REINDEX = True  # Set to False to skip LLM re-indexing if data exists\n",
    "GRAPH_PATH = \"historical_graph.json\"\n",
    "ENTITIES_PATH = \"known_entities.json\"\n",
    "\n",
    "# === Setup containers ===\n",
    "G = nx.MultiDiGraph()\n",
    "all_triples = []\n",
    "entity_usage = defaultdict(int)\n",
    "known_entities = set()\n",
    "MAX_ENTITIES_FOR_PROMPT = 40\n",
    "\n",
    "if not REINDEX and os.path.exists(GRAPH_PATH) and os.path.exists(ENTITIES_PATH):\n",
    "    print(\"ðŸ” Loading graph and entities from disk...\")\n",
    "    with open(GRAPH_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        G = json_graph.node_link_graph(data, directed=True, multigraph=True)\n",
    "    with open(ENTITIES_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        known_entities = set(json.load(f))\n",
    "    print(f\"âœ… Loaded graph with {len(G.nodes)} nodes and {len(G.edges)} edges.\")\n",
    "else:\n",
    "    print(\"âš™ï¸ Rebuilding graph from documents using LLM...\")\n",
    "    for i, doc in enumerate(documents):\n",
    "        start = time.time()\n",
    "\n",
    "        sorted_entities = sorted(known_entities, key=lambda e: -entity_usage[e])\n",
    "        limited_entities = sorted_entities[:MAX_ENTITIES_FOR_PROMPT]\n",
    "        known_str = \", \".join(limited_entities) if limited_entities else \"(none)\"\n",
    "\n",
    "        output = dynamic_entity_chain.invoke({\"input\": doc.page_content, \"known\": known_str})\n",
    "        print(f\"\\n--- LLM Output for doc {i} ---\\n{output.strip()}\")\n",
    "\n",
    "        sections = output.strip().split(\"New Entities:\")\n",
    "        triples_block = sections[0].replace(\"Triples:\", \"\").strip()\n",
    "        new_entities_block = sections[1].strip() if len(sections) > 1 else \"\"\n",
    "\n",
    "        triples = []\n",
    "        for line in triples_block.split(\"\\n\"):\n",
    "            if line.strip():\n",
    "                parts = [p.strip(\" ()\\n\") for p in line.split(\",\")]\n",
    "                if len(parts) == 3:\n",
    "                    triples.append(tuple(parts))\n",
    "                else:\n",
    "                    print(f\"âš ï¸ Skipping malformed triple line: {line}\")\n",
    "\n",
    "        # new_entities = [e.strip(\" \\\"'\") for e in new_entities_block.split(\"\\n\") if e.strip()]\n",
    "        # Normalize new entities by stripping leading/trailing spaces and quotes\n",
    "        new_entities = [e.lstrip(\"- \").strip(\" \\\"'\\n\") for e in new_entities_block.split(\"\\n\") if e.strip()]\n",
    "\n",
    "\n",
    "        all_triples.extend(triples)\n",
    "        known_entities.update(new_entities)\n",
    "\n",
    "        for s, r, o in triples:\n",
    "            # Normalize subject and object by stripping leading/trailing spaces and hyphens\n",
    "            s = s.lstrip(\"- \").strip()\n",
    "            r = r.strip(\" .-\").lower()\n",
    "            o = o.lstrip(\"- \").strip()\n",
    "            G.add_node(s)\n",
    "            G.add_node(o)\n",
    "            G.add_edge(s, o, relation=r)\n",
    "            entity_usage[s] += 1\n",
    "            entity_usage[o] += 1\n",
    "\n",
    "        print(f\"â±ï¸ Processed doc {i} in {round(time.time() - start, 2)}s\")\n",
    "\n",
    "    # === Save graph and entity index ===\n",
    "    print(\"ðŸ’¾ Saving graph and entities to disk...\")\n",
    "    with open(GRAPH_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_graph.node_link_data(G), f, ensure_ascii=False, indent=2)\n",
    "    with open(ENTITIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sorted(list(known_entities)), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… Graph built with {len(G.nodes)} nodes, {len(G.edges)} edges, and {len(known_entities)} tracked entities.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa2c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_graph(anchor_entity: str, depth: int = 2) -> nx.MultiDiGraph:\n",
    "    if anchor_entity not in G:\n",
    "        raise ValueError(f\"Entity '{anchor_entity}' not found in the graph.\")\n",
    "\n",
    "    # Use BFS to find all nodes within the specified depth\n",
    "    bfs_edges = nx.bfs_edges(G, source=anchor_entity, depth_limit=depth)\n",
    "    nodes_in_scope = {anchor_entity}\n",
    "    edges_in_scope = []\n",
    "\n",
    "    for u, v in bfs_edges:\n",
    "        nodes_in_scope.update([u, v])\n",
    "        edges_in_scope.append((u, v))\n",
    "\n",
    "    # Create a new subgraph from the collected nodes and edges\n",
    "    subgraph = nx.MultiDiGraph()\n",
    "    for u, v in edges_in_scope:\n",
    "        for key in G[u][v]:\n",
    "            relation = G[u][v][key].get(\"relation\", \"\")\n",
    "            subgraph.add_edge(u, v, relation=relation)\n",
    "            subgraph.add_node(u)\n",
    "            subgraph.add_node(v)\n",
    "\n",
    "    return subgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e26724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_graph(anchor: str, depth: int = 2) -> nx.Graph:\n",
    "    if anchor not in G:\n",
    "        raise ValueError(f\"Entity '{anchor}' not found in the graph.\")\n",
    "\n",
    "    undirected_G = G.to_undirected()\n",
    "    visited_nodes = set([anchor])\n",
    "    current_level = set([anchor])\n",
    "\n",
    "    for _ in range(depth):\n",
    "        next_level = set()\n",
    "        for node in current_level:\n",
    "            neighbors = undirected_G.neighbors(node)\n",
    "            for neighbor in neighbors:\n",
    "                if neighbor not in visited_nodes:\n",
    "                    next_level.add(neighbor)\n",
    "        visited_nodes.update(next_level)\n",
    "        current_level = next_level\n",
    "\n",
    "    subgraph = G.subgraph(visited_nodes).copy()\n",
    "    return subgraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd55f8",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Anchor Detection from Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e53d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anchor_entity(question: str) -> str:\n",
    "    MAX_ENTITIES_FOR_ANCHOR_PROMPT = 40\n",
    "\n",
    "    # Use global variables for known_entities and entity_usage\n",
    "    sorted_entities = sorted(known_entities, key=lambda e: -entity_usage.get(e, 0))\n",
    "    limited_entities = sorted_entities[:MAX_ENTITIES_FOR_ANCHOR_PROMPT]\n",
    "    known_entities_list = \"\\n\".join(f\"- {e}\" for e in limited_entities) if limited_entities else \"(none)\"\n",
    "\n",
    "    detect_prompt = PromptTemplate.from_template(f\"\"\"\n",
    "You are a semantic matcher. Your task is to identify the main entity (person, event, or discovery) from the list below that the question is primarily about.\n",
    "\n",
    "Only return the exact entity name from the list. Do not explain.\n",
    "\n",
    "Known Entities:\n",
    "{known_entities_list}\n",
    "\n",
    "Question: {{question}}\n",
    "\"\"\")\n",
    "\n",
    "    detect_chain = detect_prompt | llm | StrOutputParser()\n",
    "    return detect_chain.invoke({\"question\": question}).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10532525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_subgraph(query: str, depth: int = 2):\n",
    "    anchor_entity = detect_anchor_entity(query)\n",
    "    print(f\"ðŸ” Detected anchor entity: {anchor_entity}\")\n",
    "\n",
    "    if anchor_entity not in G:\n",
    "        print(f\"âš ï¸ Entity '{anchor_entity}' not found in the graph.\")\n",
    "        return\n",
    "\n",
    "    # Extract the subgraph based on depth\n",
    "    visited = set()\n",
    "    queue = [(anchor_entity, 0)]\n",
    "    sub_nodes = set()\n",
    "\n",
    "    while queue:\n",
    "        current_node, current_depth = queue.pop(0)\n",
    "        if current_depth > depth or current_node in visited:\n",
    "            continue\n",
    "        visited.add(current_node)\n",
    "        sub_nodes.add(current_node)\n",
    "        neighbors = list(G.successors(current_node)) + list(G.predecessors(current_node))\n",
    "        for neighbor in neighbors:\n",
    "            queue.append((neighbor, current_depth + 1))\n",
    "            sub_nodes.add(neighbor)\n",
    "\n",
    "    SG = G.subgraph(sub_nodes)\n",
    "\n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    pos = nx.spring_layout(SG, seed=42)\n",
    "    nx.draw_networkx_nodes(SG, pos, node_size=600, node_color=\"#FFEEEE\")\n",
    "    nx.draw_networkx_edges(SG, pos, arrows=True, arrowstyle='-|>', edge_color=\"#666\")\n",
    "    nx.draw_networkx_labels(SG, pos, font_size=10)\n",
    "    edge_labels = {(u, v): data[\"relation\"] for u, v, data in SG.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(SG, pos, edge_labels=edge_labels, font_size=9)\n",
    "    plt.title(f\"Subgraph for: '{anchor_entity}'\", fontsize=14)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b33879",
   "metadata": {},
   "source": [
    "## ðŸ¤” Ask a Question Using the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74468c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_with_graph(query: str, depth: int = 2):\n",
    "    anchor_entity = detect_anchor_entity(query)\n",
    "    print(f\"ðŸ” Detected anchor entity: {anchor_entity}\")\n",
    "\n",
    "    subgraph = search_graph(anchor_entity, depth=depth)\n",
    "    context = [f\"{u} {data['relation']} {v}.\" for u, v, data in subgraph.edges(data=True)]\n",
    "    context_text = \"\\n\".join(context)\n",
    "\n",
    "    response = llm.invoke(f\"Answer the question based on these facts:\\n\\n{context_text}\\n\\nQuestion: {query}\")\n",
    "\n",
    "    return context, response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888dfb88",
   "metadata": {},
   "source": [
    "## ðŸš€ Try It Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f13422",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How did Albert Einstein contribute to the development of the atomic bomb?\"\n",
    "# Set the depth for the subgraph search\n",
    "depth= 2\n",
    "retrieved_chunks, response = answer_question_with_graph(question, depth)\n",
    "\n",
    "print(\"\\nRetrieved Chunks:\")\n",
    "for line in retrieved_chunks:\n",
    "    print(\"-\", line)\n",
    "\n",
    "print(\"\\nAnswer:\")\n",
    "print(response)\n",
    "\n",
    "visualize_subgraph(question, depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa80f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(G.nodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7798e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, v, data in G.edges(data=True):\n",
    "    if \"relativity\" in u.lower() or \"relativity\" in v.lower():\n",
    "        print(f\"{u} --{data['relation']}--> {v}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
